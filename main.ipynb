{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie genre classification\n",
    "Find movie genre by its Wikipedia plot description.\n",
    "\n",
    "Dataset - [CMU Movie Summary Corpus](http://www.cs.cmu.edu/~ark/personas/). Unpack `movie.metadata.tsv` and `plot_summaries.txt` files to the root folder of this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "If you want to skip this step, you may download the [preprocessed dataset](https://drive.google.com/file/d/1U-CGGvlc3z3ayk2_qd-uOyUPiNkvsSjj/view?usp=sharing) and put the `preprocessed_dataset.csv` into the root folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Check for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if not exists('movie.metadata.tsv'):\n",
    "    raise Exception('movie.metadata.tsv was not found')\n",
    "\n",
    "if not exists('plot_summaries.txt'):\n",
    "    raise Exception('plot_summaries.txt was not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Load `movie.metadata.tsv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "meta = pd.read_csv(\"movie.metadata.tsv\", sep = '\\t', header = None)\n",
    "meta.columns = ['Movie ID', 1, 2, 3, 4, 5, 6, 7, 'Genre']\n",
    "\n",
    "# remove unnecessary columns\n",
    "meta = meta.drop(columns=[1, 2, 3, 4, 5, 6, 7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3. Genres are initially string type. Convert them to a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "genres = []\n",
    "\n",
    "for i in meta[\"Genre\"]:\n",
    "    genres.append(list(json.loads(i).values()))\n",
    "\n",
    "meta[\"Genre\"] = genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.4. Overall, there are 363 genres in the dataset. We do not need so much. We reduced the number of genres to 19. After reducing, some movies are left without genres at all, so we remove such movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_genre(genre):\n",
    "    if genre == \"Black-and-white\":\n",
    "        return \"Monochrome\"\n",
    "    if genre == \"Family Film\":\n",
    "        return \"Family\"\n",
    "    if genre == \"War film\":\n",
    "        return \"War\"\n",
    "    if genre == \"Biographical film\":\n",
    "        return \"Biography\"\n",
    "    if genre == \"Crime Fiction\":\n",
    "        return \"Crime\"\n",
    "    if genre == \"Romance Film\":\n",
    "        return \"Romance\"\n",
    "    return genre\n",
    "\n",
    "all_general_genres = [\n",
    "    'Animation',\n",
    "    'Black-and-white',\n",
    "    'Short Film',\n",
    "    'Documentary',\n",
    "    'Family Film',\n",
    "    'Adventure',\n",
    "    'Action',\n",
    "    'Thriller',\n",
    "    'Drama'\n",
    "]\n",
    "\n",
    "all_specific_genres = [\n",
    "    'Horror',\n",
    "    'Musical',\n",
    "    'Fantasy',\n",
    "    'War film',\n",
    "    'Biography',\n",
    "    'Biographical film',\n",
    "    'Science Fiction',\n",
    "    'Crime Fiction',\n",
    "    'Romance Film',\n",
    "    'Mystery',\n",
    "    'Comedy'\n",
    "]\n",
    "\n",
    "meta_reduced_general_genres = []\n",
    "meta_reduced_specific_genres = []\n",
    "\n",
    "# reduce all movies genres by picking one general and one specific genre. Since the above genre arrays are sorted by priority, we select the first genre that is present both in the movie genre list and in the above array\n",
    "for i in meta['Genre']:\n",
    "    meta_general_genre = \"\"\n",
    "    meta_specific_genre = \"\"\n",
    "    for general_genre in all_general_genres:\n",
    "        if general_genre in i:\n",
    "            meta_general_genre = general_genre\n",
    "            break\n",
    "\n",
    "    for specific_genre in all_specific_genres:\n",
    "        if specific_genre in i:\n",
    "            meta_specific_genre = specific_genre\n",
    "            break\n",
    "    meta_reduced_general_genres.append(parse_genre(meta_general_genre))\n",
    "    meta_reduced_specific_genres.append(parse_genre(meta_specific_genre))\n",
    "\n",
    "# add two new columns: general and specific genres\n",
    "meta[\"General genre\"] = meta_reduced_general_genres\n",
    "meta[\"Specific genre\"] = meta_reduced_specific_genres\n",
    "\n",
    "# remove movies without genres\n",
    "meta = meta[~(meta['General genre'].str.len() == 0)]\n",
    "meta = meta[~(meta['Specific genre'].str.len() == 0)]\n",
    "\n",
    "# remove genre column\n",
    "meta = meta.drop(columns=\"Genre\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5. Load `plot_summaries.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42303it [00:03, 13084.90it/s]\n",
      "100%|██████████| 42303/42303 [00:00<00:00, 899449.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "plot_summaries = []\n",
    "movie_ids = []\n",
    "plots = []\n",
    "\n",
    "with open(\"plot_summaries.txt\", 'r', encoding=\"utf8\") as file:\n",
    "       reader = csv.reader(file, dialect='excel-tab') \n",
    "       for row in tqdm(reader):\n",
    "            plot_summaries.append(row)\n",
    "            \n",
    "for p in tqdm(plot_summaries):\n",
    "    movie_ids.append(p[0])\n",
    "    plots.append(p[1])\n",
    "\n",
    "plot_data = {'Movie ID': movie_ids, 'Plot' : plots}\n",
    "plot_df = pd.DataFrame(plot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.6. Merge movies names and genres with their plot descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = []\n",
    "meta['Movie ID'] = meta['Movie ID'].astype(str)\n",
    "\n",
    "# merge two tables according to id\n",
    "merged = pd.merge(plot_df, meta[['Movie ID', 'General genre', 'Specific genre']], on = ['Movie ID'])\n",
    "\n",
    "def change_column_order(df, col_name, index):\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(col_name)\n",
    "    cols.insert(index, col_name)\n",
    "    return df[cols]\n",
    "\n",
    "merged = change_column_order(merged, 'Plot', 1)\n",
    "merged = change_column_order(merged, 'General genre', 2)\n",
    "merged = change_column_order(merged, 'Specific genre', 3)\n",
    "merged = merged.drop(columns=\"Movie ID\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.7. Clean plot descriptions by removing unnecessary symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "new_plot_summaries = []\n",
    "\n",
    "def simplify_text(text):\n",
    "    text = re.sub('\\'','', text)\n",
    "    text = re.sub('[^a-zA-Z]',' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# clean the plot\n",
    "merged['Plot Updated'] = merged['Plot'].apply(lambda l: simplify_text(l))\n",
    "merged['Plot'] = merged['Plot Updated'].values\n",
    "\n",
    "merged = merged.drop(columns='Plot Updated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.8. Clean plot descriptions by removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Akmal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# download the list of stopwords from the nltk library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for x in words:\n",
    "        if x not in stop_words:\n",
    "            new_text.append(x)\n",
    "    return ' '.join(new_text)\n",
    "\n",
    "# remove stop words from the plot\n",
    "merged['Plot'] = merged['Plot'].apply(lambda l: remove_stopwords(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.9. Save preprocessed data to the `preprocessed_dataset.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe as csv file\n",
    "merged.to_csv('preprocessed_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Check for dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import exists\n",
    "\n",
    "if not exists('preprocessed_dataset.csv'):\n",
    "    raise Exception('preprocessed_dataset.csv was not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2. Load `preprocessed_dataset.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "movies = pd.read_csv(\"preprocessed_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Split dataset to training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data to train and test sets\n",
    "general_xtrain, general_xtest, general_ytrain, general_ytest = train_test_split(movies['Plot'], movies[\"General genre\"], test_size=0.2, random_state=42)\n",
    "specific_xtrain, specific_xtest, specific_ytrain, specific_ytest = train_test_split(movies['Plot'], movies[\"Specific genre\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.4. Extract features from plots (appr. 50-60 seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "## Create vectorizer\n",
    "general_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=15000, max_df=0.8)\n",
    "specific_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=15000, max_df=0.8)\n",
    "\n",
    "# create TF-IDF features\n",
    "general_xtrain_tfidf = general_vectorizer.fit_transform(general_xtrain)\n",
    "general_xtest_tfidf = general_vectorizer.transform(general_xtest)\n",
    "specific_xtrain_tfidf = specific_vectorizer.fit_transform(specific_xtrain)\n",
    "specific_xtest_tfidf = specific_vectorizer.transform(specific_xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1. HyperParameter optimization (Note: if you run this block, the next step will run for more than an hour due to the optimization. But you can skip this step, then the next step will just use hardcoded models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    " \n",
    "# defining parameter range\n",
    "param_grid = {'C': [0.00001, 0.0001, 0.001, 0.08, 0.09, 0.1, 0.2, 0.3, 1, 10, 100],\n",
    "              'tol': [0.00001, 0.0001, 0.001, 0.08, 0.09, 0.1],\n",
    "              'class_weight': ['balanced', None]}\n",
    " \n",
    "general_svc_optimized = GridSearchCV(LinearSVC(random_state=42), param_grid, refit = True, verbose = 3)\n",
    "specific_svc_optimized = GridSearchCV(LinearSVC(random_state=42), param_grid, refit = True, verbose = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2. Model training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "if 'general_svc_optimized' in locals():\n",
    "    # use hyperoptimized models\n",
    "    general_svc_optimized.fit(general_xtrain_tfidf, general_ytrain)\n",
    "    specific_svc_optimized.fit(specific_xtrain_tfidf, specific_ytrain)\n",
    "\n",
    "    general_y_pred = general_svc_optimized.predict(general_xtest_tfidf)\n",
    "    specific_y_pred = specific_svc_optimized.predict(specific_xtest_tfidf)\n",
    "else:\n",
    "    # use hardcoded models (parameters were obtained by hyperoptimization)\n",
    "    general_svc = LinearSVC(C=0.8, class_weight='balanced', tol=0.5, random_state=42)\n",
    "    specific_svc = LinearSVC(C=0.2, tol=0.5, random_state=42)\n",
    "\n",
    "    general_svc.fit(general_xtrain_tfidf, general_ytrain)\n",
    "    specific_svc.fit(specific_xtrain_tfidf, specific_ytrain)\n",
    "\n",
    "    general_y_pred = general_svc.predict(general_xtest_tfidf)\n",
    "    specific_y_pred = specific_svc.predict(specific_xtest_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Value metrics\n",
    "print(classification_report(general_ytest, general_y_pred))\n",
    "print(classification_report(specific_ytest, specific_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2. Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion matrices\n",
    "general_cm = confusion_matrix(general_ytest, general_y_pred, labels=general_svc.classes_)\n",
    "specific_cm = confusion_matrix(specific_ytest, specific_y_pred, labels=specific_svc.classes_)\n",
    "\n",
    "general_cm_plot = ConfusionMatrixDisplay(confusion_matrix=general_cm, display_labels=general_svc.classes_)\n",
    "specific_cm_plot = ConfusionMatrixDisplay(confusion_matrix=specific_cm, display_labels=specific_svc.classes_)\n",
    "\n",
    "general_cm_plot.plot()\n",
    "plt.xticks(rotation = 90)\n",
    "specific_cm_plot.plot()\n",
    "plt.xticks(rotation = 90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Empirical tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1. Define helper functions (if you skipped part 1 - preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def simplify_text(text):\n",
    "    text = re.sub('\\'','', text)\n",
    "    text = re.sub('[^a-zA-Z]',' ', text)\n",
    "    text = ' '.join(text.split())\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# download the list of stopwords from the nltk library\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    new_text = []\n",
    "    for x in words:\n",
    "        if x not in stop_words:\n",
    "            new_text.append(x)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2. Define test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_plot(plot):\n",
    "    plot = simplify_text(plot)\n",
    "    plot = remove_stopwords(plot)\n",
    "    return plot\n",
    "\n",
    "def find_general_genre(plot):\n",
    "    q_vec = general_vectorizer.transform([plot])\n",
    "    general_genre_pred = general_svc.predict(q_vec)\n",
    "    return general_genre_pred\n",
    "\n",
    "def find_specific_genre(plot):\n",
    "    q_vec = specific_vectorizer.transform([plot])\n",
    "    specific_genre_pred = specific_svc.predict(q_vec)\n",
    "    return specific_genre_pred\n",
    "\n",
    "def find_genres(plot):\n",
    "    plot = clean_plot(plot)\n",
    "    general_genre = find_general_genre(plot)\n",
    "    specific_genre = find_specific_genre(plot)\n",
    "    return general_genre + \", \" + specific_genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3. Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Action, Crime']\n"
     ]
    }
   ],
   "source": [
    "# Movie: The Batman (2022)\n",
    "# Real genres: Action, Crime, Superhero, Mystery, Drama, Adventure\n",
    "plot = \"On Halloween, Gotham City mayor Don Mitchell Jr. is murdered by a man calling himself the Riddler. Reclusive billionaire Bruce Wayne, who has operated for two years as the vigilante Batman, investigates alongside the Gotham City Police Department (GCPD). Lieutenant James Gordon discovers a message that the Riddler left for Batman. The Riddler kills commissioner Pete Savage and leaves another message for Batman. Batman and Gordon discover that the Riddler left a thumb drive in Mitchell's car containing images of Mitchell with a woman, Annika Koslov, at the Iceberg Lounge—a nightclub operated by the Penguin, mobster Carmine Falcone's lieutenant. While the Penguin pleads ignorance, Batman notices that Selina Kyle, Annika's roommate and friend, works at the club as a waitress. When Annika disappears, Batman sends Selina back to the Iceberg Lounge for answers and discovers that Savage was on Falcone's payroll, as is district attorney Gil Colson. The Riddler abducts Colson, straps a timed collar bomb to his neck, and sends him to interrupt Mitchell's funeral. When Batman arrives, the Riddler calls him via Colson's phone and threatens to detonate the bomb if Colson cannot answer three riddles. Colson refuses to answer the third—the name of the informant who gave the GCPD information that led to a historic drug bust ending mobster Salvatore Maroni's operation—and dies. Batman and Gordon deduce that the informant may be the Penguin and track him to a drug deal. They discover that Maroni's operation transferred to Falcone, with many corrupt GCPD officers involved. Selina inadvertently exposes them when she arrives to steal money and discovers Annika's corpse in a car trunk. After a car chase, Batman captures the Penguin but learns he is not the informant. Batman and Gordon follow the Riddler's trail to the ruins of an orphanage funded by Bruce's murdered parents, Thomas and Martha Wayne, where they learn that the Riddler holds a grudge against the Wayne family. Bruce's butler and caretaker, Alfred Pennyworth, is hospitalized after opening a letter bomb addressed to Bruce. The Riddler leaks evidence that Thomas, who was running for mayor before he was murdered, hired Falcone to kill a journalist for threatening to reveal details about Martha and her family's history of mental illness. Bruce, who grew up believing his father was morally upstanding, confronts Alfred, who maintains that Thomas only asked Falcone to threaten the journalist into silence; Thomas planned to turn himself and Falcone over to the police once he found out Falcone murdered the journalist instead. Alfred believes that Falcone had Thomas and Martha killed to prevent this. Selina reveals to Batman that Falcone is her neglectful father. She decides to kill him after learning that he strangled Annika because Mitchell told her that Falcone was the informant. Batman and Gordon arrive in time to stop her, but the Riddler kills Falcone as he is being arrested. The Riddler is unmasked as forensic accountant Edward Nashton and is incarcerated in Arkham State Hospital, where he tells Batman he took inspiration from him when targeting the corrupt. Batman learns that Nashton has stationed car bombs around Gotham and cultivated an online following that plans to assassinate mayor-elect Bella Reál. The bombs destroy the seawall around Gotham and flood the city. Nashton's followers attempt to kill Reál but are stopped by Batman and Selina. In the aftermath, Nashton befriends another inmate, while Selina deems Gotham beyond saving and leaves. Batman aids recovery efforts and vows to inspire hope in Gotham.\"\n",
    "print(find_genres(plot))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "adffd716f13183dc292e66097959b728d09c186bf7d88a0919d1fa9c383260b7"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('movie-genres-9WvDornV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
